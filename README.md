# TRPO from Scratch â€” Solving Humanoid-v5

  

"From line search headaches to teaching humanoids to walk â€” this is TRPO, rebuilt from the ground up."

## ğŸš€ Overview

This repository contains a from-scratch implementation of Trust Region Policy Optimization (TRPO) in PyTorch. Unlike typical black-box libraries, this implementation includes all core TRPO components â€” manually coded, debugged, and optimized to solve the complex Humanoid-v5 environment.

    ğŸ” Line Search with backtracking
    
    ğŸ¯ KL Constraint enforcement (trust region)
    
    ğŸ“ˆ Generalized Advantage Estimation (GAE)
    
    â™»ï¸ Conjugate Gradient Solver
    
    ğŸ”¥ Entropy Regularization (tunable)
    
    Final performance: ~1360 normalized reward on Humanoid-v5.

## ğŸ§  Core Components

    âœ”ï¸ Policy Network (Stochastic)
    
    Outputs mean and std of Gaussian policy with tanh squashing.
    
    âœ”ï¸ Value Network
    
    Critic network used for estimating state value functions.
    
    âœ”ï¸ GAE (Generalized Advantage Estimation)
    
    Stabilizes training by balancing bias and variance in advantage calculation.
    
    âœ”ï¸ Conjugate Gradient Solver
    
    Used to approximate natural gradients under the Fisher Information Matrix.
    
    âœ”ï¸ Line Search with KL Constraint
    
    Ensures policy updates stay within a trusted region to avoid divergence.
    
    âœ”ï¸ Entropy Bonus
    
    Encourages exploration. Fully tunable (e.g., entropy_coef=0.002).

### ğŸ› ï¸ Training Setup

Environment: Humanoid-v5

Framework: PyTorch

Training episodes: Up to 3000+

Average reward achieved: 1300+

Max reward observed: ~1362

## ğŸ“Š Sample Logs

Success: 2241
Line search success: True
Episode 2339 | Total Reward: 136.22 | Max reward: 136.21
...
Episode 3000 | Total Reward: 11.24 | Max reward: 136.21

ğŸ“¦ Installation

git clone https://github.com/yourusername/trpo-humanoid-v5.git
cd trpo-humanoid-v5


ğŸƒâ€â™‚ï¸ Run Training

python train_trpo.py --env Humanoid-v5 --episodes 3000 --entropy_coef 0.002 --max_kl 0.2

You can modify training hyperparameters in config.py or directly in the training script.

## Reward Graph
![8cd32900-50fe-45f8-8f0c-8c95421cda29](https://github.com/user-attachments/assets/0a41aef1-507e-49d2-98d6-522c51fde647)




ğŸ’¡ Credits

Inspired by the original TRPO paper by Schulman et al., but written completely from scratch by Brijesh, with a mission to truly understand and rebuild RL from the ground up.

ğŸ“œ License

MIT License

Future work will include Integrating TRPO with World Model, a hybrid MBPO style which works in latent space with trust region which be faster and more powerful then any past models made till now.
